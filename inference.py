import ast
import json
import os
import pdb
from collections.abc import Mapping
import pandas as pd
from torchvision.transforms import InterpolationMode

import torch
from vision_process import process_vision_info, smart_resize
from torchvision import io, transforms

from data import DataConfig
from utils import ModelConfig, PEFTLoraConfig, TrainingConfig
from utils import load_model_from_checkpoint
from train_reward import create_model_and_processor
from prompt_template import build_prompt


def load_configs_from_json(config_path):
    with open(config_path, "r") as f:
        config_dict = json.load(f)

    # del config_dict["training_args"]["_n_gpu"]
    del config_dict["data_config"]["meta_data"]
    del config_dict["data_config"]["data_dir"]

    return config_dict["data_config"], None, config_dict["model_config"], config_dict["peft_lora_config"], \
           config_dict["inference_config"] if "inference_config" in config_dict else None

class VideoVLMRewardInference():
    def __init__(self, load_from_pretrained, load_from_pretrained_step=-1, device='cuda', dtype=torch.bfloat16):
        config_path = os.path.join(load_from_pretrained, "model_config.json")
        data_config, _, model_config, peft_lora_config, inference_config = load_configs_from_json(config_path)
        data_config = DataConfig(**data_config)
        model_config = ModelConfig(**model_config)
        peft_lora_config = PEFTLoraConfig(**peft_lora_config)

        training_args = TrainingConfig(
            load_from_pretrained=load_from_pretrained,
            load_from_pretrained_step=load_from_pretrained_step,
            gradient_checkpointing=False,
            disable_flash_attn2=False,
            bf16=True if dtype == torch.bfloat16 else False,
            fp16=True if dtype == torch.float16 else False,
            output_dir="",
        )
        
        model, processor, peft_config = create_model_and_processor(
            model_config=model_config,
            peft_lora_config=peft_lora_config,
            training_args=training_args,
        )

        self.device = device

        model, checkpoint_step = load_model_from_checkpoint(model, load_from_pretrained, load_from_pretrained_step)
        model.eval()

        self.model = model
        self.processor = processor

        self.model.to(self.device)

        self.data_config = data_config

        self.inference_config = inference_config

    def debug_print_shapes(self, data, name="Data", indent=0):
        """
        é€’å½’æ‰“å°æ•°æ®ç»“æ„ï¼ŒåŒ…æ‹¬å¼ é‡çš„å½¢çŠ¶ã€å‡å€¼ã€æ–¹å·®ã€æå€¼å’Œéƒ¨åˆ†å–å€¼ã€‚
        """
        tab = "  " * indent
        if indent == 0:
            print(f"\n{'#'*30} DEBUG: {name} {'#'*30}")

        if isinstance(data, (dict, Mapping)):
            print(f"{tab}Dict (keys: {list(data.keys())})")
            for k, v in data.items():
                print(f"{tab}Key: '{k}'")
                self.debug_print_shapes(v, name=k, indent=indent + 1)

        elif isinstance(data, (list, tuple)):
            print(f"{tab}List/Tuple (length: {len(data)})")
            if len(data) > 0:
                # å¦‚æœæ˜¯åˆ—è¡¨ä¸”å…ƒç´ å¾ˆå¤šï¼Œåªå±•ç¤ºç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯ï¼Œé¿å…åˆ·å±
                if len(data) > 2:
                    print(f"{tab}  [Showing first item]:")
                    self.debug_print_shapes(data[0], name="list[0]", indent=indent + 1)
                    print(f"{tab}  ...")
                    print(f"{tab}  [Showing last item]:")
                    self.debug_print_shapes(data[-1], name=f"list[{len(data)-1}]", indent=indent + 1)
                else:
                    for i, item in enumerate(data):
                        self.debug_print_shapes(item, name=f"list[{i}]", indent=indent + 1)

        elif isinstance(data, torch.Tensor):
            # è·å–åŸºæœ¬å±æ€§
            shape = list(data.shape)
            dtype = data.dtype
            device = data.device
            
            # è®¡ç®—ç»Ÿè®¡é‡ (ä»…å¯¹æ•°å€¼å‹å¼ é‡)
            with torch.no_grad():
                if data.numel() > 0 and torch.is_floating_point(data):
                    v_min = data.min().item()
                    v_max = data.max().item()
                    v_mean = data.mean().item()
                    v_std = data.std().item()
                elif data.numel() > 0: # æ•´æ•°ç±»å‹ (å¦‚ input_ids)
                    v_min = data.min().item()
                    v_max = data.max().item()
                    v_mean = data.float().mean().item()
                    v_std = data.float().std().item()
                else:
                    v_min = v_max = v_mean = v_std = 0

            # è·å–å‰å‡ ä¸ªæ•°æ®ç‚¹ä½œä¸ºä¾‹å­
            flat_data = data.flatten()
            sample_size = min(5, flat_data.numel())
            samples = flat_data[:sample_size].tolist()
            sample_str = ", ".join([f"{x:.4f}" if isinstance(x, float) else str(x) for x in samples])

            print(f"{tab}Tensor Shape: {shape}")
            print(f"{tab}  - Dtype: {dtype} | Device: {device}")
            print(f"{tab}  - Stats: Min: {v_min:.4f}, Max: {v_max:.4f}, Mean: {v_mean:.4f}, Std: {v_std:.4f}")
            print(f"{tab}  - Samples: [{sample_str}...]")

        else:
            print(f"{tab}Value: {data} (Type: {type(data)})")

        if indent == 0:
            print(f"{'#'*80}\n")
    def _norm(self, reward):
        if self.inference_config is None:
            return reward
        else:
            reward['VQ'] = (reward['VQ'] - self.inference_config['VQ_mean']) / self.inference_config['VQ_std']
            reward['MQ'] = (reward['MQ'] - self.inference_config['MQ_mean']) / self.inference_config['MQ_std']
            reward['TA'] = (reward['TA'] - self.inference_config['TA_mean']) / self.inference_config['TA_std']
            return reward

    def _pad_sequence(self, sequences, attention_mask, max_len, padding_side='right'):
        """
        Pad the sequences to the maximum length.
        """
        assert padding_side in ['right', 'left']
        if sequences.shape[1] >= max_len:
            return sequences, attention_mask
        
        pad_len = max_len - sequences.shape[1]
        padding = (0, pad_len) if padding_side == 'right' else (pad_len, 0)

        sequences_padded = torch.nn.functional.pad(sequences, padding, 'constant', self.processor.tokenizer.pad_token_id)
        attention_mask_padded = torch.nn.functional.pad(attention_mask, padding, 'constant', 0)

        return sequences_padded, attention_mask_padded
    
    def _prepare_input(self, data):
        """
        Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and
        handling potential state.
        """
        if isinstance(data, Mapping):
            return type(data)({k: self._prepare_input(v) for k, v in data.items()})
        elif isinstance(data, (tuple, list)):
            return type(data)(self._prepare_input(v) for v in data)
        elif isinstance(data, torch.Tensor):
            kwargs = {"device": self.device}
            ## TODO: Maybe need to add dtype
            # if self.is_deepspeed_enabled and (torch.is_floating_point(data) or torch.is_complex(data)):
            #     # NLP models inputs are int/uint and those get adjusted to the right dtype of the
            #     # embedding. Other models such as wav2vec2's inputs are already float and thus
            #     # may need special handling to match the dtypes of the model
            #     kwargs.update({"dtype": self.accelerator.state.deepspeed_plugin.hf_ds_config.dtype()})
            return data.to(**kwargs)
        return data
    
    def _prepare_inputs(self, inputs):
        """
        Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and
        handling potential state.
        """
        inputs = self._prepare_input(inputs)
        if len(inputs) == 0:
            raise ValueError
        return inputs
    
    def prepare_batch(self, video_paths, prompts, fps=None, num_frames=None, max_pixels=None,):
        fps = self.data_config.fps if fps is None else fps
        num_frames = self.data_config.num_frames if num_frames is None else num_frames
        max_pixels = self.data_config.max_frame_pixels if max_pixels is None else max_pixels

        if num_frames is None:
            chat_data = [
                [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "video", 
                                "video": f"file://{video_path}", 
                                "max_pixels": max_pixels, 
                                "fps": fps,
                                "sample_type": self.data_config.sample_type,
                            },
                            {"type": "text", "text": build_prompt(prompt, self.data_config.eval_dim, self.data_config.prompt_template_type)},
                        ],
                    },
                ] for video_path, prompt in zip(video_paths, prompts)
            ]
        else:
            chat_data = [
                [
                    {
                        "role": "user",
                        "content": [
                            {
                                "type": "video",
                                "video": f"file://{video_path}", 
                                "max_pixels": max_pixels, 
                                "nframes": num_frames,
                                "sample_type": self.data_config.sample_type,
                            },
                            {"type": "text", "text": build_prompt(prompt, self.data_config.eval_dim, self.data_config.prompt_template_type)},
                        ],
                    },
                ] for video_path, prompt in zip(video_paths, prompts)
            ]
        image_inputs, video_inputs = process_vision_info(chat_data)
        print(">>> æ£€æŸ¥ process_vision_info è¾“å‡º:")
        self.debug_print_shapes(image_inputs, "image_inputs")
        self.debug_print_shapes(video_inputs, "video_inputs")

        batch = self.processor(
            text=self.processor.apply_chat_template(chat_data, tokenize=False, add_generation_prompt=True),
            images=image_inputs,
            videos=video_inputs,
            padding=True,
            return_tensors="pt",
            videos_kwargs={"do_rescale": True},
        )
        print(">>> æ£€æŸ¥ self.processor åçš„ batch (CPU):")
        self.debug_print_shapes(batch, "batch_pre_move")
        batch = self._prepare_inputs(batch)
                # --- è°ƒè¯•ç‚¹ 3: _prepare_inputs åçš„ batch (æ­¤æ—¶åœ¨ GPU ä¸Š) ---
        print(">>> æ£€æŸ¥ self._prepare_inputs åçš„ batch (GPU):")
        self.debug_print_shapes(batch, "batch_post_move")
        return batch

    def reward(self, video_paths, prompts, fps=None, num_frames=None, max_pixels=None, use_norm=True):
        """
        Inputs:
            video_paths: List[str], B paths of the videos.
            prompts: List[str], B prompts for the videos.
            eval_dims: List[str], N evaluation dimensions.
            fps: float, sample rate of the videos. If None, use the default value in the config.
            num_frames: int, number of frames of the videos. If None, use the default value in the config.
            max_pixels: int, maximum pixels of the videos. If None, use the default value in the config.
            use_norm: bool, whether to rescale the output rewards
        Outputs:
            Rewards: List[dict], N + 1 rewards of the B videos.
        """
        assert fps is None or num_frames is None, "fps and num_frames cannot be set at the same time."
        
        batch = self.prepare_batch(video_paths, prompts, fps, num_frames, max_pixels)
        rewards = self.model(
            return_dict=True,
            **batch
        )["logits"]
        print("Raw rewards:", rewards)

        rewards = [{'VQ': reward[0].item(), 'MQ': reward[1].item(), 'TA': reward[2].item()} for reward in rewards]
        for i in range(len(rewards)):
            if use_norm:
                rewards[i] = self._norm(rewards[i])
            rewards[i]['Overall'] = rewards[i]['VQ'] + rewards[i]['MQ'] + rewards[i]['TA']

        return rewards

import torch
import torch.nn.functional as F

import torch

import torch
import itertools

import torch
import torch.nn.functional as F
import itertools
from vision_process import process_vision_info

import torch

import torch
import torch.nn.functional as F
import itertools
from vision_process import process_vision_info

import torch
import torch.nn.functional as F
import itertools
from vision_process import process_vision_info

import torch
import torch.nn.functional as F

def differentiable_process_vlm_video_v446(video_tensor, processor):
    """
    ä¸¥æ ¼æŒ‰ç…§ Qwen2-VL v4.46.2 å®˜æ–¹ 9 ç»´æ‹†è§£é€»è¾‘å®ç°
    video_tensor: [T, C, H, W], float32, [0, 255]
    """
    # 1. å½’ä¸€åŒ–å‚æ•°è·å–
    image_processor = processor.image_processor
    mean = torch.tensor(image_processor.image_mean).view(1, 3, 1, 1).to(video_tensor.device)
    std = torch.tensor(image_processor.image_std).view(1, 3, 1, 1).to(video_tensor.device)

    # 2. å½’ä¸€åŒ–
    x = video_tensor / 255.0
    x = (x - mean) / std

    # 3. åŸºç¡€ç»´åº¦å®šä¹‰
    T, C, H, W = x.shape
    patch_size = 14
    temporal_patch_size = 2
    merge_size = 2  # å®˜æ–¹æ ¸å¿ƒå‚æ•°
    
    grid_t = T // temporal_patch_size
    grid_h = H // patch_size
    grid_w = W // patch_size

    # 4. 9 ç»´æ‹†è§£ (ä¸¥æ ¼å¯¹é½å®˜æ–¹ reshape é€»è¾‘)
    # å®˜æ–¹é¡ºåºï¼šgrid_t, ts, channel, gh//m, m, ps, gw//m, m, ps
    # æ³¨æ„ï¼šPyTorch é»˜è®¤æ˜¯ C-firstï¼Œæ‰€ä»¥ view é¡ºåºéœ€æå…¶ç²¾ç¡®
    x = x.view(
        grid_t,
        temporal_patch_size,
        C,
        grid_h // merge_size,
        merge_size,
        patch_size,
        grid_w // merge_size,
        merge_size,
        patch_size,
    )

    # å®˜æ–¹ä»£ç ï¼špatches.transpose(0, 3, 6, 4, 7, 2, 1, 5, 8)
    # å¯¹åº”ç‰©ç†å«ä¹‰ï¼š
    # 0: grid_t
    # 3: grid_h // 2
    # 6: grid_w // 2
    # 4: merge_h (2)
    # 7: merge_w (2)
    # 2: channel (3)
    # 1: ts (2)
    # 5: patch_h (14)
    # 8: patch_w (14)
    x = x.permute(0, 3, 6, 4, 7, 2, 1, 5, 8).contiguous()

    # å¤–éƒ¨ï¼šgrid_t * (grid_h/2) * (grid_w/2) * 2 * 2 = grid_t * grid_h * grid_w
    # å†…éƒ¨ï¼š3 * 2 * 14 * 14 = 1176
    pixel_values_videos = x.view(-1, C * temporal_patch_size * patch_size * patch_size)
    
    return pixel_values_videos

import torch
import torch.nn.functional as F
from vision_process import process_vision_info

def verify_v446_consistency(inferencer, video_path, prompt):
    print(">>> æ­£åœ¨è·å–å®˜æ–¹è¾“å‡º...")
    # 1. å®˜æ–¹ Processor å¤„ç†
    batch_official = inferencer.prepare_batch([video_path], [prompt], num_frames=10)
    gt_pixels = batch_official['pixel_values_videos'].cpu().float()
    grid_thw = batch_official['video_grid_thw'][0].tolist()
    
    # 2. è·å–åŸå§‹è¾“å…¥å¹¶ Resize åˆ°å®˜æ–¹ç›®æ ‡å°ºå¯¸
    target_h, target_w = grid_thw[1] * 14, grid_thw[2] * 14
    chat_data = [[{"role": "user", "content": [{"type": "video", "video": f"file://{video_path}", "nframes": 10, "sample_type": inferencer.data_config.sample_type}, {"type": "text", "text": prompt}]}]]
    _, video_inputs = process_vision_info(chat_data)
    video_tensor_raw = video_inputs[0].clone().detach().float().to(inferencer.device)
    
    # ç¡®ä¿å°ºå¯¸å®Œå…¨ä¸€è‡´ï¼ˆç”±å®˜æ–¹ Grid åæ¨ï¼‰
    video_tensor_raw = F.interpolate(video_tensor_raw, size=(target_h, target_w), mode='bicubic', align_corners=False)
    video_tensor_raw.requires_grad = True
    print(video_tensor_raw.shape)

    # 3. è¿è¡Œæ–°ç‰ˆ 9 ç»´å¯¹é½å‡½æ•°
    print(">>> è¿è¡Œ 9 ç»´å¯¹é½å¯å¾®å¤„ç†...")
    my_pixels = differentiable_process_vlm_video_v446(video_tensor_raw, inferencer.processor)
    my_pixels_cpu = my_pixels.detach().cpu()

    # 4. å¯¹æ¯”ç»“æœ
    mse = torch.mean((gt_pixels - my_pixels_cpu)**2).item()
    max_diff = torch.max(torch.abs(gt_pixels - my_pixels_cpu)).item()
    
    print("\n" + "="*50)
    print(f"MSE: {mse:.10f}")
    print(f"Max Diff: {max_diff:.10f}")
    
    if mse < 1e-5:
        print("âœ… å®Œç¾å¯¹é½ï¼ç»ˆäºæ‰¾åˆ°äº†å®˜æ–¹ v4.46.2 çš„çœŸå®é€»è¾‘ã€‚")
        # æ¢¯åº¦æµ‹è¯•
        my_pixels.sum().backward()
        print(f"âœ… æ¢¯åº¦æ£€æŸ¥é€šè¿‡: Grad Mean = {video_tensor_raw.grad.abs().mean().item():.6f}")
    else:
        print("âŒ ä¾ç„¶ä¸å¯¹ï¼Œè¯·æ£€æŸ¥å®˜æ–¹ Processor çš„å…·ä½“ç‰ˆæœ¬æ˜¯å¦å¸¦æœ‰å…¶ä»–è‡ªå®šä¹‰è½¬æ¢ã€‚")
        print(f"å®˜æ–¹å‰5: {gt_pixels[0, :5].tolist()}")
        print(f"æ‰‹åŠ¨å‰5: {my_pixels_cpu[0, :5].tolist()}")
    print("="*50)
from torchvision import io


def get_mock_vae_output(video_path, num_frames=100, height=480, width=832, device="cuda"):
    """
    è¯»å–è§†é¢‘å¹¶è½¬æ¢ä¸ºæ¨¡æ‹Ÿ VAE Decoder è¾“å‡ºçš„å¼ é‡ [-1, 1]
    """
    # 1. è¯»å–è§†é¢‘å¸§ [T, H, W, C]
    # pts_unit='sec' ç¡®ä¿æ—¶é—´æˆ³å¯¹é½
    vframes, _, _ = io.read_video(video_path, pts_unit='sec', output_format="TCHW")

    # 2. å‡åŒ€é‡‡æ ·å¸§æ•°
    total_frames = vframes.shape[0]
    indices = torch.linspace(0, total_frames - 1, steps=num_frames).long()
    sampled_frames = vframes[indices] # [num_frames, C, H, W]

    # 3. è½¬æ¢ä¸º float32 å¹¶ Resize åˆ° VAE é€šå¸¸è¾“å‡ºçš„åˆ†è¾¨ç‡
    # æ³¨æ„ï¼šVAE è¾“å‡ºçš„åˆ†è¾¨ç‡é€šå¸¸æ˜¯ 8 çš„å€æ•°
    video_tensor = sampled_frames.to(device).float()

    video_tensor = F.interpolate(video_tensor, size=(height, width), mode='bicubic', align_corners=False)

    # 4. æ ¸å¿ƒæ­¥éª¤ï¼šå½’ä¸€åŒ–åˆ° [-1, 1]
    # å…¬å¼ï¼š(x / 127.5) - 1.0
    video_vae_style = (video_tensor / 127.5) - 1.0

    # 5. å¼€å¯æ¢¯åº¦ï¼Œæ¨¡æ‹Ÿè®­ç»ƒçŠ¶æ€
    video_vae_style.requires_grad = True

    return video_vae_style


def get_video_tensor_for_reward(video_path, num_frames, max_pixels, sample_type="uniform", device="cuda"):
    """
    ä¸¥æ ¼æŒ‰ç…§å®˜æ–¹ fetch_video é€»è¾‘è¯»å–è§†é¢‘ï¼Œè¿”å›å¯å¾®åˆ†çš„å¼ é‡ã€‚

    è¿™ä¸ªå‡½æ•°çš„è¾“å‡ºä¸ process_vision_info å®Œå…¨ä¸€è‡´ï¼Œä½†ä¿æŒ requires_grad=Trueã€‚

    Args:
        video_path: è§†é¢‘è·¯å¾„
        num_frames: é‡‡æ ·å¸§æ•°
        max_pixels: æ¯å¸§æœ€å¤§åƒç´ æ•°
        sample_type: é‡‡æ ·ç±»å‹ ("uniform" æˆ– "multi_pts")
        device: è®¾å¤‡

    Returns:
        video_tensor: [T, C, H, W] float32 å¼ é‡ï¼ŒèŒƒå›´ [0, 255]ï¼Œå¯æ±‚æ¢¯åº¦
        resized_height: resize åçš„é«˜åº¦
        resized_width: resize åçš„å®½åº¦
    """
    from vision_process import smart_resize, round_by_factor, FRAME_FACTOR

    # 1. è¯»å–è§†é¢‘
    vframes, _, info = io.read_video(video_path, pts_unit='sec', output_format="TCHW")
    total_frames = vframes.shape[0]
    video_fps = info.get("video_fps", 30.0)

    # 2. å¸§æ•°å¤„ç† - ä¸¥æ ¼å¯¹é½å®˜æ–¹é€»è¾‘
    nframes = round_by_factor(num_frames, FRAME_FACTOR)
    if nframes > total_frames:
        nframes = total_frames

    # 3. å¸§é‡‡æ · - ä½¿ç”¨ .round().long() ä¸å®˜æ–¹ä¸€è‡´
    if sample_type == 'uniform':
        idx = torch.linspace(0, total_frames - 1, nframes).round().long().tolist()
    elif sample_type == 'multi_pts':
        frames_each_pts = 6
        num_pts = 4
        fps = 8
        nframes_temp = int(total_frames * fps // video_fps)
        frames_idx = torch.linspace(0, total_frames - 1, nframes_temp).round().long().tolist()
        start_pt = int(frames_each_pts // 2)
        end_pt = int(nframes_temp - frames_each_pts // 2 - 1)
        pts = torch.linspace(start_pt, end_pt, num_pts).round().long().tolist()
        idx = []
        for pt in pts:
            idx.extend(frames_idx[pt - frames_each_pts // 2 : pt + frames_each_pts // 2])
    else:
        idx = torch.linspace(0, total_frames - 1, nframes).round().long().tolist()

    video = vframes[idx]  # [T, C, H, W]
    nframes, _, height, width = video.shape

    # 4. è®¡ç®— resize å°ºå¯¸ - ä¸¥æ ¼æŒ‰ç…§ fetch_video é€»è¾‘
    VIDEO_MIN_PIXELS = 128 * 28 * 28
    VIDEO_MAX_PIXELS = 768 * 28 * 28
    VIDEO_TOTAL_PIXELS = 24576 * 28 * 28

    min_pixels = VIDEO_MIN_PIXELS
    total_pixels = VIDEO_TOTAL_PIXELS
    computed_max_pixels = max(min(VIDEO_MAX_PIXELS, total_pixels / nframes * FRAME_FACTOR), int(min_pixels * 1.05))
    final_max_pixels = max_pixels if max_pixels is not None else computed_max_pixels

    resized_height, resized_width = smart_resize(
        height, width,
        factor=28,
        min_pixels=min_pixels,
        max_pixels=final_max_pixels,
    )

    # 5. Resize - ä¸å®˜æ–¹å®Œå…¨ä¸€è‡´
    video_tensor = transforms.functional.resize(
        video,
        [resized_height, resized_width],
        interpolation=InterpolationMode.BICUBIC,
        antialias=True,
    ).float().to(device)

    # 6. å¼€å¯æ¢¯åº¦
    video_tensor.requires_grad = True

    return video_tensor, resized_height, resized_width


def vae_output_to_pixel_values(vae_output, target_height, target_width, processor):
    """
    å°† VAE è¾“å‡º ([-1, 1]) è½¬æ¢ä¸º VLM æ‰€éœ€çš„ pixel_valuesã€‚

    å®Œæ•´çš„å¯å¾®åˆ†æµæ°´çº¿ï¼šVAE output -> resize -> normalize -> 9ç»´æ‹†è§£

    Args:
        vae_output: [T, C, H, W] å¼ é‡ï¼ŒèŒƒå›´ [-1, 1]ï¼Œæ¥è‡ª VAE decoder
        target_height: ç›®æ ‡é«˜åº¦ (å¿…é¡»æ˜¯ 28 çš„å€æ•°)
        target_width: ç›®æ ‡å®½åº¦ (å¿…é¡»æ˜¯ 28 çš„å€æ•°)
        processor: Qwen2-VL processor

    Returns:
        pixel_values_videos: [N, 1176] æ ¼å¼çš„ token å¼ é‡
    """
    # 1. è½¬å› [0, 255] èŒƒå›´
    video_255 = (vae_output + 1.0) * 127.5

    # 2. Resize åˆ°ç›®æ ‡å°ºå¯¸ (åªåšä¸€æ¬¡ resize)
    video_resized = transforms.functional.resize(
        video_255,
        [target_height, target_width],
        interpolation=InterpolationMode.BICUBIC,
        antialias=True,
    )

    # 3. 9 ç»´å¯¹é½å¤„ç†
    pixel_values = differentiable_process_vlm_video_v446(video_resized, processor)

    return pixel_values


import torch
import torch.nn.functional as F
from torchvision import transforms
from torchvision.transforms import InterpolationMode

def compare_manual_vs_official_rewards(inferencer, video_path, prompt):
    print("\n" + "="*25 + " å¼€å§‹åŒè·¯å¾„å¯¹æ¯”éªŒè¯ " + "="*25)
    device = inferencer.device
    model_dtype = inferencer.model.dtype

    # ---------------------------------------------------------
    # ç¬¬ä¸€éƒ¨åˆ†ï¼šå®˜æ–¹è·¯å¾„ (Official Path)
    # ---------------------------------------------------------
    with torch.no_grad():
        # 1. ä½¿ç”¨å®˜æ–¹ processor å¾—åˆ° batch
        batch_official = inferencer.prepare_batch([video_path], [prompt], num_frames=10)
        # 2. å®˜æ–¹æ¨ç†å¾—åˆ°å¥–åŠ±
        rewards_official = inferencer.model(return_dict=True, **batch_official)["logits"]
    
    gt_pixels = batch_official['pixel_values_videos'].float().cpu()
    grid_thw = batch_official['video_grid_thw']

    # ---------------------------------------------------------
    # ç¬¬äºŒéƒ¨åˆ†ï¼šæ‰‹åŠ¨å¯å¾®è·¯å¾„ (Manual Differentiable Path)
    # ---------------------------------------------------------
    # 1. æ¨¡æ‹Ÿ VAE è¾“å‡ºå¹¶è½¬å› [0, 255]
    # æ³¨æ„ï¼šä¸ºäº†å…¬å¹³å¯¹æ¯”ï¼Œæˆ‘ä»¬ç›´æ¥ä» get_mock_vae_output æ‹¿åˆ°å¼ é‡
    vae_output = get_mock_vae_output(video_path, num_frames=10)
    vae_output_255 = (vae_output + 1) / 2 * 255.0
    
    # 2. ä¸¥æ ¼æŒ‰ç…§å®˜æ–¹ Grid è¿›è¡Œ Resize
    target_h, target_w = grid_thw[0, 1].item() * 14, grid_thw[0, 2].item() * 14
    video_resized = transforms.functional.resize(
        vae_output_255,
        [target_h, target_w],
        interpolation=InterpolationMode.BICUBIC,
        antialias=True
    )

    # 3. è¿è¡Œ 9 ç»´å¯¹é½çš„å¯å¾®å¤„ç†å‡½æ•°
    pixel_values_manual = differentiable_process_vlm_video_v446(video_resized, inferencer.processor)
    
    # 4. æ„é€ æ‰‹åŠ¨ Batch å¹¶æ¨ç†
    # æˆ‘ä»¬ä¿ç•™æ¢¯åº¦ï¼Œæ¨¡æ‹ŸçœŸå®è®­ç»ƒåœºæ™¯
    input_batch_manual = {
        "input_ids": batch_official["input_ids"],
        "attention_mask": batch_official["attention_mask"],
        "video_grid_thw": batch_official["video_grid_thw"],
        "pixel_values_videos": pixel_values_manual.to(model_dtype)
    }
    
    # æ‰‹åŠ¨è·¯å¾„æ¨ç†
    outputs_manual = inferencer.model(return_dict=True, **input_batch_manual)
    rewards_manual = outputs_manual["logits"]

    # ---------------------------------------------------------
    # ç¬¬ä¸‰éƒ¨åˆ†ï¼šå·®å¼‚åˆ†æ
    # ---------------------------------------------------------
    # 1. Pixel å·®å¼‚
    pixel_manual_cpu = pixel_values_manual.detach().float().cpu()
    pixel_mse = torch.mean((gt_pixels - pixel_manual_cpu)**2).item()
    pixel_max_diff = torch.max(torch.abs(gt_pixels - pixel_manual_cpu)).item()

    # 2. Reward Logits å·®å¼‚ (VQ, MQ, TA)
    # å°†ç»“æœè½¬å› float32 æ–¹ä¾¿å¯¹æ¯”
    res_off = rewards_official.float().cpu()[0]
    res_man = rewards_manual.detach().float().cpu()[0]
    logit_diff = torch.abs(res_off - res_man)

    print("\n[ç»´åº¦éªŒè¯]")
    print(f"Pixel Values Shape: {gt_pixels.shape}")
    print(f"Grid THW: {grid_thw.tolist()}")

    print("\n[1. è¾“å…¥ç‰¹å¾ Pixel Values å·®å¼‚]")
    print(f"MSE: {pixel_mse:.10f}")
    print(f"Max Absolute Diff: {pixel_max_diff:.10f}")

    print("\n[2. æœ€ç»ˆ Reward Logits (æœªå½’ä¸€åŒ–) å¯¹æ¯”]")
    dims = ['VQ', 'MQ', 'TA']
    for i, name in enumerate(dims):
        print(f"{name} Dimension: Official={res_off[i]:.4f} | Manual={res_man[i]:.4f} | Diff={logit_diff[i]:.4f}")

    reward_mse = torch.mean(logit_diff**2).item()
    print(f"\nReward Logits Total MSE: {reward_mse:.10e}")

    if reward_mse < 1e-3:
        print("\nâœ… éªŒè¯é€šè¿‡ï¼šæ‰‹åŠ¨è·¯å¾„æ¨ç†ç»“æœä¸å®˜æ–¹åŸç‰ˆå‡ ä¹æ— å¼‚ï¼Œå¯ä»¥æ”¾å¿ƒç”¨äºè®­ç»ƒã€‚")
    else:
        print("\nâš ï¸ è­¦å‘Šï¼šæ¨ç†ç»“æœå­˜åœ¨åå·®ï¼Œè¯·æ£€æŸ¥ Resize é€»è¾‘æˆ–å½’ä¸€åŒ–å‚æ•°ã€‚")

    return rewards_manual

import random
from tqdm import tqdm


def batch_validate_reward_consistency_v2(inferencer, video_dir, num_samples=100):
    """
    ä¿®å¤åçš„æ‰¹é‡éªŒè¯å‡½æ•° - ç¡®ä¿å®˜æ–¹è·¯å¾„å’Œæ‰‹åŠ¨è·¯å¾„å®Œå…¨ä¸€è‡´ã€‚

    æ ¸å¿ƒä¿®å¤ï¼š
    1. ä½¿ç”¨ get_video_tensor_for_reward ä¸å®˜æ–¹å¸§é‡‡æ ·å’Œ resize é€»è¾‘å®Œå…¨ä¸€è‡´
    2. é¿å…ä¸¤æ¬¡ resize å¯¼è‡´çš„ç´¯ç§¯è¯¯å·®
    3. ä½¿ç”¨ .round().long() ä¸å®˜æ–¹å¸§ç´¢å¼•è®¡ç®—ä¸€è‡´
    """
    print(f"\nå¼€å§‹æ‰¹é‡éªŒè¯ (V2 - ä¿®å¤ç‰ˆ)... ç›®æ ‡æ ·æœ¬æ•°: {num_samples}")

    all_videos = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]
    if len(all_videos) < num_samples:
        print(f"è­¦å‘Š: è§†é¢‘æ•°é‡ä¸è¶³ {num_samples}ï¼Œå°†å¤„ç†æ‰€æœ‰ {len(all_videos)} ä¸ªè§†é¢‘ã€‚")
        sample_videos = all_videos
    else:
        sample_videos = random.sample(all_videos, num_samples)

    results = []
    num_frames = inferencer.data_config.num_frames if inferencer.data_config.num_frames else 10
    max_pixels = inferencer.data_config.max_frame_pixels

    for vid_name in tqdm(sample_videos):
        video_path = os.path.join(video_dir, vid_name)
        prompt = "The video shows natural movement and high quality scene."

        try:
            with torch.no_grad():
                # --- å®˜æ–¹è·¯å¾„ ---
                batch_off = inferencer.prepare_batch([video_path], [prompt], num_frames=num_frames)
                res_off = inferencer.model(**batch_off)["logits"].float().cpu()[0]
                gt_pixels = batch_off['pixel_values_videos'].float().cpu()

            # --- æ‰‹åŠ¨å¯å¾®è·¯å¾„ (ä½¿ç”¨ä¿®å¤åçš„å‡½æ•°) ---
            # 1. ä½¿ç”¨ä¸å®˜æ–¹å®Œå…¨ä¸€è‡´çš„è§†é¢‘è¯»å–å’Œ resize é€»è¾‘
            video_tensor, resized_h, resized_w = get_video_tensor_for_reward(
                video_path,
                num_frames=num_frames,
                max_pixels=max_pixels,
                sample_type=inferencer.data_config.sample_type,
                device=inferencer.device
            )

            # 2. ç›´æ¥è¿›è¡Œ 9 ç»´å¤„ç† (ä¸éœ€è¦é¢å¤– resizeï¼Œå› ä¸º get_video_tensor_for_reward å·²ç»å¤„ç†å¥½)
            pixel_values_manual = differentiable_process_vlm_video_v446(video_tensor, inferencer.processor)

            # 3. æ„é€  batch å¹¶æ¨ç†
            batch_man = {
                "input_ids": batch_off["input_ids"],
                "attention_mask": batch_off["attention_mask"],
                "video_grid_thw": batch_off["video_grid_thw"],
                "pixel_values_videos": pixel_values_manual.to(inferencer.model.dtype)
            }

            with torch.no_grad():
                res_man = inferencer.model(**batch_man)["logits"].float().cpu()[0]

            # 4. æ£€æŸ¥ pixel å·®å¼‚
            pixel_manual_cpu = pixel_values_manual.detach().float().cpu()
            pixel_mse = torch.mean((gt_pixels - pixel_manual_cpu)**2).item()

            # 5. è®°å½•æ•°æ®
            diff = torch.abs(res_off - res_man)
            results.append({
                'video': vid_name,
                'pixel_mse': pixel_mse,
                'off_VQ': res_off[0].item(), 'man_VQ': res_man[0].item(),
                'off_MQ': res_off[1].item(), 'man_MQ': res_man[1].item(),
                'off_TA': res_off[2].item(), 'man_TA': res_man[2].item(),
                'diff_VQ': diff[0].item(), 'diff_MQ': diff[1].item(), 'diff_TA': diff[2].item(),
                'sign_match_VQ': (res_off[0] * res_man[0] > 0).item(),
                'sign_match_MQ': (res_off[1] * res_man[1] > 0).item(),
                'sign_match_TA': (res_off[2] * res_man[2] > 0).item()
            })

        except Exception as e:
            print(f"è·³è¿‡è§†é¢‘ {vid_name}, é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

    df = pd.DataFrame(results)

    print("\n" + "="*20 + " æ‰¹é‡éªŒè¯ç»Ÿè®¡æŠ¥å‘Š (V2) " + "="*20)
    stats = {
        "å¹³å‡ Pixel MSE": df['pixel_mse'].mean(),
        "å¹³å‡ç»å¯¹è¯¯å·® (MAE) - VQ": df['diff_VQ'].mean(),
        "å¹³å‡ç»å¯¹è¯¯å·® (MAE) - MQ": df['diff_MQ'].mean(),
        "å¹³å‡ç»å¯¹è¯¯å·® (MAE) - TA": df['diff_TA'].mean(),
        "æ­£è´Ÿå·ä¸€è‡´ç‡ (VQ)": df['sign_match_VQ'].mean(),
        "æ­£è´Ÿå·ä¸€è‡´ç‡ (MQ)": df['sign_match_MQ'].mean(),
        "æ­£è´Ÿå·ä¸€è‡´ç‡ (TA)": df['sign_match_TA'].mean(),
        "æœ€å¤§ç»å¯¹åå·® (MaxDiff)": df[['diff_VQ', 'diff_MQ', 'diff_TA']].max().max()
    }

    for k, v in stats.items():
        print(f"{k:30}: {v:.6f}")

    print("\n[ç›¸å…³æ€§åˆ†æ]")
    print(f"VQ ç›¸å…³ç³»æ•°: {df['off_VQ'].corr(df['man_VQ']):.4f}")
    print(f"MQ ç›¸å…³ç³»æ•°: {df['off_MQ'].corr(df['man_MQ']):.4f}")
    print(f"TA ç›¸å…³ç³»æ•°: {df['off_TA'].corr(df['man_TA']):.4f}")

    return df


def batch_validate_reward_consistency(inferencer, video_dir, num_samples=100):
    """
    åŸå§‹éªŒè¯å‡½æ•° - ä¿ç•™ç”¨äºå¯¹æ¯”ï¼Œä½†æ¨èä½¿ç”¨ batch_validate_reward_consistency_v2
    """
    print(f"\nå¼€å§‹æ‰¹é‡éªŒè¯... ç›®æ ‡æ ·æœ¬æ•°: {num_samples}")

    # 1. è·å–ç›®å½•ä¸‹æ‰€æœ‰è§†é¢‘æ–‡ä»¶
    all_videos = [f for f in os.listdir(video_dir) if f.endswith(('.mp4', '.avi', '.mov'))]
    if len(all_videos) < num_samples:
        print(f"è­¦å‘Š: è§†é¢‘æ•°é‡ä¸è¶³ {num_samples}ï¼Œå°†å¤„ç†æ‰€æœ‰ {len(all_videos)} ä¸ªè§†é¢‘ã€‚")
        sample_videos = all_videos
    else:
        sample_videos = random.sample(all_videos, num_samples)

    # 2. åˆå§‹åŒ–ç»Ÿè®¡åˆ—è¡¨
    results = []

    # 3. å¾ªç¯æµ‹è¯•
    for vid_name in tqdm(sample_videos):
        video_path = os.path.join(video_dir, vid_name)
        # è¿™é‡Œå¯ä»¥ä½¿ç”¨åŠ¨æ€ Promptï¼Œæˆ–è€…å›ºå®šä¸€ä¸ªé€šç”¨ Prompt
        prompt = "The video shows natural movement and high quality scene."

        try:
            # ä½¿ç”¨ä¹‹å‰çš„å¯¹æ¯”é€»è¾‘è·å–ç»“æœ (æ³¨æ„: compare_manual_vs_official_rewards éœ€è¦ç¨å¾®æ”¹åŠ¨è¿”å› diff æ•°æ®)
            # ä¸ºäº†åŠ é€Ÿï¼Œæˆ‘ä»¬åœ¨ compare å†…éƒ¨ç”¨ torch.no_grad()
            with torch.no_grad():
                # --- å®˜æ–¹è·¯å¾„ ---
                batch_off = inferencer.prepare_batch([video_path], [prompt], num_frames=10)
                res_off = inferencer.model(**batch_off)["logits"].float().cpu()[0]

                # --- æ‰‹åŠ¨è·¯å¾„ ---
                # æ¨¡æ‹Ÿ VAE è¾“å‡º
                vae_out = get_mock_vae_output(video_path, num_frames=10)
                vae_255 = (vae_out + 1) / 2 * 255.0

                # åŒ¹é…å°ºå¯¸å¹¶ Resize
                grid_thw = batch_off['video_grid_thw'][0]
                th, tw = grid_thw[1] * 14, grid_thw[2] * 14
                v_res = transforms.functional.resize(vae_255, [th, tw],
                                                     interpolation=InterpolationMode.BICUBIC, antialias=True)

                # 9ç»´å¤„ç†
                pix_man = differentiable_process_vlm_video_v446(v_res, inferencer.processor)

                # æ¨ç†
                batch_man = {
                    "input_ids": batch_off["input_ids"],
                    "attention_mask": batch_off["attention_mask"],
                    "video_grid_thw": batch_off["video_grid_thw"],
                    "pixel_values_videos": pix_man.to(inferencer.model.dtype)
                }
                res_man = inferencer.model(**batch_man)["logits"].float().cpu()[0]

            # 4. è®°å½•æ•°æ®
            diff = torch.abs(res_off - res_man)
            results.append({
                'video': vid_name,
                'off_VQ': res_off[0].item(), 'man_VQ': res_man[0].item(),
                'off_MQ': res_off[1].item(), 'man_MQ': res_man[1].item(),
                'off_TA': res_off[2].item(), 'man_TA': res_man[2].item(),
                'diff_VQ': diff[0].item(), 'diff_MQ': diff[1].item(), 'diff_TA': diff[2].item(),
                'sign_match_VQ': (res_off[0] * res_man[0] > 0).item(),
                'sign_match_MQ': (res_off[1] * res_man[1] > 0).item(),
                'sign_match_TA': (res_off[2] * res_man[2] > 0).item()
            })

        except Exception as e:
            print(f"è·³è¿‡è§†é¢‘ {vid_name}, é”™è¯¯: {e}")

    # 5. èšåˆç»Ÿè®¡åˆ†æ
    df = pd.DataFrame(results)

    print("\n" + "="*20 + " æ‰¹é‡éªŒè¯ç»Ÿè®¡æŠ¥å‘Š " + "="*20)
    stats = {
        "å¹³å‡ç»å¯¹è¯¯å·® (MAE) - VQ": df['diff_VQ'].mean(),
        "å¹³å‡ç»å¯¹è¯¯å·® (MAE) - MQ": df['diff_MQ'].mean(),
        "å¹³å‡ç»å¯¹è¯¯å·® (MAE) - TA": df['diff_TA'].mean(),
        "æ­£è´Ÿå·ä¸€è‡´ç‡ (VQ)": df['sign_match_VQ'].mean(),
        "æ­£è´Ÿå·ä¸€è‡´ç‡ (MQ)": df['sign_match_MQ'].mean(),
        "æ­£è´Ÿå·ä¸€è‡´ç‡ (TA)": df['sign_match_TA'].mean(),
        "æœ€å¤§ç»å¯¹åå·® (MaxDiff)": df[['diff_VQ', 'diff_MQ', 'diff_TA']].max().max()
    }

    for k, v in stats.items():
        print(f"{k:25}: {v:.4f}")

    # ç›¸å…³æ€§åˆ†æ (Pearson Correlation)
    print("\n[ç›¸å…³æ€§åˆ†æ]")
    print(f"VQ ç›¸å…³ç³»æ•°: {df['off_VQ'].corr(df['man_VQ']):.4f}")
    print(f"MQ ç›¸å…³ç³»æ•°: {df['off_MQ'].corr(df['man_MQ']):.4f}")
    print(f"TA ç›¸å…³ç³»æ•°: {df['off_TA'].corr(df['man_TA']):.4f}")

    return df

class DifferentiableVideoReward:
    """
    å¯å¾®åˆ†çš„è§†é¢‘å¥–åŠ±æ¨ç†ç±»ï¼Œç”¨äº RL è®­ç»ƒã€‚

    ä½¿ç”¨æ–¹æ³•ï¼š
    ```python
    reward_model = DifferentiableVideoReward("./checkpoints", device="cuda:0")

    # ä» VAE decoder è¾“å‡ºè®¡ç®—å¥–åŠ± (ä¿æŒæ¢¯åº¦)
    vae_output = vae.decode(latent)  # [-1, 1] èŒƒå›´
    reward = reward_model.compute_reward_from_vae_output(
        vae_output,
        prompt="A cat walking on grass",
        target_height=336,  # å¿…é¡»æ˜¯ 28 çš„å€æ•°
        target_width=504,   # å¿…é¡»æ˜¯ 28 çš„å€æ•°
    )
    # reward å½¢çŠ¶: [1, 3] (VQ, MQ, TA)
    # å¯ä»¥ç›´æ¥ .backward()
    loss = -reward.sum()
    loss.backward()
    ```
    """

    def __init__(self, load_from_pretrained, load_from_pretrained_step=-1, device='cuda', dtype=torch.bfloat16):
        self.inferencer = VideoVLMRewardInference(
            load_from_pretrained,
            load_from_pretrained_step=load_from_pretrained_step,
            device=device,
            dtype=dtype
        )
        self.device = device
        self.dtype = dtype

    def compute_reward_from_vae_output(self, vae_output, prompt, target_height, target_width):
        """
        ä» VAE è¾“å‡ºè®¡ç®—å¯å¾®åˆ†çš„å¥–åŠ±ã€‚

        Args:
            vae_output: [T, C, H, W] å¼ é‡ï¼ŒèŒƒå›´ [-1, 1]ï¼Œæ¥è‡ª VAE decoder
            prompt: æ–‡æœ¬æç¤º
            target_height: ç›®æ ‡é«˜åº¦ (å¿…é¡»æ˜¯ 28 çš„å€æ•°)
            target_width: ç›®æ ‡å®½åº¦ (å¿…é¡»æ˜¯ 28 çš„å€æ•°)

        Returns:
            rewards: [1, 3] å¼ é‡ (VQ, MQ, TA)ï¼Œä¿æŒæ¢¯åº¦
        """
        assert target_height % 28 == 0, f"target_height å¿…é¡»æ˜¯ 28 çš„å€æ•°ï¼Œå½“å‰ä¸º {target_height}"
        assert target_width % 28 == 0, f"target_width å¿…é¡»æ˜¯ 28 çš„å€æ•°ï¼Œå½“å‰ä¸º {target_width}"

        # 1. è½¬æ¢ VAE è¾“å‡ºåˆ° pixel_values
        pixel_values = vae_output_to_pixel_values(
            vae_output,
            target_height,
            target_width,
            self.inferencer.processor
        )

        # 2. è®¡ç®— video_grid_thw
        T = vae_output.shape[0]
        grid_t = T // 2  # temporal_patch_size = 2
        grid_h = target_height // 14  # patch_size = 14
        grid_w = target_width // 14
        video_grid_thw = torch.tensor([[grid_t, grid_h, grid_w]], device=self.device)

        # 3. è·å–æ–‡æœ¬ tokens
        text_batch = self._prepare_text_tokens(prompt)

        # 4. æ„é€ å®Œæ•´ batch
        batch = {
            "input_ids": text_batch["input_ids"],
            "attention_mask": text_batch["attention_mask"],
            "video_grid_thw": video_grid_thw,
            "pixel_values_videos": pixel_values.to(self.dtype)
        }

        # 5. å‰å‘ä¼ æ’­
        outputs = self.inferencer.model(return_dict=True, **batch)
        rewards = outputs["logits"]

        return rewards

    def _prepare_text_tokens(self, prompt):
        """å‡†å¤‡æ–‡æœ¬ tokens (ä¸åŒ…å«è§†é¢‘)"""
        from prompt_template import build_prompt

        eval_dim = self.inferencer.data_config.eval_dim
        prompt_template_type = self.inferencer.data_config.prompt_template_type

        chat_data = [[
            {
                "role": "user",
                "content": [
                    {"type": "video"},  # å ä½ç¬¦
                    {"type": "text", "text": build_prompt(prompt, eval_dim, prompt_template_type)},
                ],
            }
        ]]

        text = self.inferencer.processor.apply_chat_template(chat_data, tokenize=False, add_generation_prompt=True)
        text_tokens = self.inferencer.processor.tokenizer(
            text,
            return_tensors="pt",
            padding=True
        )

        return {
            "input_ids": text_tokens["input_ids"].to(self.device),
            "attention_mask": text_tokens["attention_mask"].to(self.device)
        }

    def compute_target_size(self, height, width, num_frames, max_pixels=None):
        """
        è®¡ç®—å®˜æ–¹çš„ç›®æ ‡ resize å°ºå¯¸ã€‚

        Args:
            height: åŸå§‹é«˜åº¦
            width: åŸå§‹å®½åº¦
            num_frames: å¸§æ•°
            max_pixels: å¯é€‰çš„æœ€å¤§åƒç´ æ•°

        Returns:
            (target_height, target_width): 28 å¯¹é½çš„ç›®æ ‡å°ºå¯¸
        """
        from vision_process import smart_resize, FRAME_FACTOR

        VIDEO_MIN_PIXELS = 128 * 28 * 28
        VIDEO_MAX_PIXELS = 768 * 28 * 28
        VIDEO_TOTAL_PIXELS = 24576 * 28 * 28

        min_pixels = VIDEO_MIN_PIXELS
        total_pixels = VIDEO_TOTAL_PIXELS
        computed_max_pixels = max(min(VIDEO_MAX_PIXELS, total_pixels / num_frames * FRAME_FACTOR), int(min_pixels * 1.05))
        final_max_pixels = max_pixels if max_pixels is not None else computed_max_pixels

        target_h, target_w = smart_resize(
            height, width,
            factor=28,
            min_pixels=min_pixels,
            max_pixels=final_max_pixels,
        )

        return target_h, target_w


if __name__ == "__main__":
    # ä½¿ç”¨ä¿®å¤åçš„ V2 éªŒè¯å‡½æ•°
    inferencer = VideoVLMRewardInference("./checkpoints", device="cuda:0")

    video_dir = "/home/wubin/wanx-code/data_mixkit/data/video"
    df_results = batch_validate_reward_consistency_v2(inferencer, video_dir, num_samples=100)

    # ä¿å­˜ç»“æœå¤‡æŸ¥
    df_results.to_csv("reward_consistency_test_v2.csv", index=False)
# # --- æ‰§è¡ŒéªŒè¯ ---
# if __name__ == "__main__":
#     # åˆå§‹åŒ–
#     from inference import VideoVLMRewardInference
#     inferencer = VideoVLMRewardInference("./checkpoints", device="cuda:0")
    
#     video_path = "/home/wubin/wanx-code/data_mixkit/data/video/mixkit-a-couple-arguing-and-struggling-42244.mp4"
#     prompt = "The camera remains still, a couple is arguing and struggling in a room."

#     # è¿è¡Œå¯¹æ¯”
#     compare_manual_vs_official_rewards(inferencer, video_path, prompt)


# # --- æµ‹è¯•ä»£ç  ---
# if __name__ == "__main__":
#     inferencer = VideoVLMRewardInference(load_from_pretrained, device=device)

#     video_path = "/home/wubin/wanx-code/data_mixkit/data/video/mixkit-a-couple-arguing-and-struggling-42244.mp4" # æ‰¾ä¸€ä¸ªå­˜åœ¨çš„è§†é¢‘
#     vae_output = get_mock_vae_output(video_path)
#     nframes, _, height, width = vae_output.shape
#     vae_output = (vae_output+1) / 2 * 255.0  # è½¬å› [0, 255] èŒƒå›´ï¼Œæ¨¡æ‹ŸåŸå§‹è§†é¢‘å¸§

#     resized_height, resized_width = smart_resize(
#             height,
#             width,
#             factor=28,
#             min_pixels=100000,
#             max_pixels=200704,
#         )
#     video_differentiable = transforms.functional.resize(
#                 vae_output,
#                 [resized_height, resized_width],
#                 interpolation=InterpolationMode.BICUBIC,
#                 antialias=True,
#             )
#     print("Original VAE Output Shape:", vae_output.shape)
#     print("Resized Video Shape:", video_differentiable.shape)
#         # 4. æ‰§è¡Œ 9 ç»´å¯¹é½çš„å¯å¾®é¢„å¤„ç†
#     # è¿™æ­¥å°† [T, C, H, W] è½¬æ¢ä¸º [Tokens, 1176]
#     pixel_values = differentiable_process_vlm_video_v446(video_differentiable, inferencer.processor)
    
#     # 5. æ„é€ æ¨¡å‹æ‰€éœ€çš„ Batch
#     # æˆ‘ä»¬éœ€è¦ä»å®˜æ–¹è·å– input_ids (æ–‡æœ¬éƒ¨åˆ†æ˜¯ä¸éœ€è¦æ¢¯åº¦çš„)
#     prompt = "The camera remains still, a couple is arguing."
#     with torch.no_grad():
#         # è·å–ä¸€ä¸ªçœŸå®çš„ batch ç”¨æ¥æ‹¿ input_ids å’Œ grid_thw
#         batch_official = inferencer.prepare_batch([video_path], [prompt], num_frames=10)
    
#     # æ„å»ºæœ€ç»ˆç”¨äºåä¼ çš„ Batch
#     # æ³¨æ„ï¼šè¦æŠŠ pixel_values æ›¿æ¢ä¸ºæˆ‘ä»¬å¸¦æ¢¯åº¦çš„ç‰ˆæœ¬ï¼Œå¹¶è½¬ä¸ºæ¨¡å‹å¯¹åº”çš„ dtype (bfloat16)
#     model_dtype = inferencer.model.dtype
#     input_batch = {
#         "input_ids": batch_official["input_ids"].to(inferencer.device),
#         "attention_mask": batch_official["attention_mask"].to(inferencer.device),
#         "video_grid_thw": batch_official["video_grid_thw"].to(inferencer.device),
#         "pixel_values_videos": pixel_values.to(model_dtype).to(inferencer.device)
#     }

#     # 6. å‰å‘ä¼ æ’­ (Reward æ¨ç†)
#     # æ³¨æ„ï¼šæ­¤å¤„ä¸èƒ½ç”¨ torch.no_grad()ï¼Œå› ä¸ºæˆ‘ä»¬è¦ç•™ç€æ¢¯åº¦
#     print(">>> æ­£åœ¨è¿›è¡Œå¯å¾®å‰å‘ä¼ æ’­...")
#     outputs = inferencer.model(return_dict=True, **input_batch)
#     logits = outputs["logits"] # å‡è®¾è¾“å‡ºå½¢çŠ¶ä¸º [1, 3] (VQ, MQ, TA)
    
#     # è®¡ç®—ä¸€ä¸ªæ ‡é‡å¥–åŠ± (ä¾‹å¦‚ä¸‰ä¸ªç»´åº¦çš„æ€»å’Œ)
#     reward_score = logits.sum()
#     print(f"Reward Score: {reward_score.item():.4f}")

#     # 7. æ¢¯åº¦å›ä¼ éªŒè¯
#     print(">>> æ­£åœ¨æ‰§è¡Œåå‘ä¼ æ’­...")
#     reward_score.backward()

#     # 8. æ£€æŸ¥æœ€åˆçš„ vae_output æ˜¯å¦æ‹¿åˆ°äº†æ¢¯åº¦
#     if vae_output.grad is not None:
#         grad_mean = vae_output.grad.abs().mean().item()
#         print(f"âœ… æ¢¯åº¦å›ä¼ æˆåŠŸ!")
#         print(f"VAE Output Grad Mean: {grad_mean:.10e}")
        
#         # ç®€å•æ£€æŸ¥æ¢¯åº¦æ˜¯å¦éé›¶
#         if grad_mean > 0:
#             print("ğŸš€ æ¢¯åº¦ä¿¡å·æœ‰æ•ˆï¼Œå¯ä»¥å¼€å§‹ RL è®­ç»ƒå¾ªç¯ã€‚")
#         else:
#             print("âš ï¸ æ¢¯åº¦ä¸ºé›¶ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ˜¯å¦å¤„äº eval() æ¨¡å¼æˆ–æ˜¯å¦æœ‰æŸäº›å±‚é˜»æ–­äº†æ¢¯åº¦ã€‚")
#     else:
#         print("âŒ æ¢¯åº¦å›ä¼ å¤±è´¥ï¼Œvae_output.grad ä¸º Noneã€‚")

#     # 9. (å¯é€‰) æ¸…ç†æ˜¾å­˜
#     del outputs, input_batch, pixel_values
#     torch.cuda.empty_cache()
    


# if __name__ == "__main__":
#     # åˆå§‹åŒ–ä½ çš„æ¨ç†ç±»
#     # æ³¨æ„ï¼šç¡®ä¿ checkpoints è·¯å¾„æ­£ç¡®
#     load_from_pretrained = "./checkpoints" 
#     device = "cuda:0"
#     inferencer = VideoVLMRewardInference(load_from_pretrained, device=device)

#     video_path = "/home/wubin/wanx-code/data_mixkit/data/video/mixkit-a-couple-arguing-and-struggling-42244.mp4" # æ‰¾ä¸€ä¸ªå­˜åœ¨çš„è§†é¢‘
#     prompt = "The video begins in a bowling alley setting, showcasing a bowling ball return machine filled with colorful balls - purple, yellow, blue, red, and green. Initially, only the person's legs and feet, clad in tan pants and red shoes, are visible, suggesting they are standing or walking in the alley. The background is adorned with the typical wooden lane surface and purple and pink lights, contributing to the alley's ambiance.\n\nAs the video progresses, the person's hands come into view, reaching into the bowling ball return machine. One hand grasps the yellow bowling ball, while the other appears to be steadying or adjusting the purple ball. The red and green balls remain undisturbed inside the machine. The person seems to be in the process of retrieving or organizing the bowling balls from the machine, with the background elements, including the wooden lane surface and colored lights, remaining consistent throughout this sequence.",


#     with torch.set_grad_enabled(True):
#         verify_v446_consistency(inferencer, video_path, prompt)

# if __name__ == "__main__":
#     load_from_pretrained = "./checkpoints"
#     device = "cuda:0"
#     dtype = torch.bfloat16

#     inferencer = VideoVLMRewardInference(load_from_pretrained, device=device, dtype=dtype)

#     video_paths = [
#         "/home/wubin/wanx-code/data_mixkit/data/video/mixkit-a-person-takes-a-bowling-ball-and-makes-a-shot-49102.mp4",
       
#     ]

#     prompts = [
#         "The video begins in a bowling alley setting, showcasing a bowling ball return machine filled with colorful balls - purple, yellow, blue, red, and green. Initially, only the person's legs and feet, clad in tan pants and red shoes, are visible, suggesting they are standing or walking in the alley. The background is adorned with the typical wooden lane surface and purple and pink lights, contributing to the alley's ambiance.\n\nAs the video progresses, the person's hands come into view, reaching into the bowling ball return machine. One hand grasps the yellow bowling ball, while the other appears to be steadying or adjusting the purple ball. The red and green balls remain undisturbed inside the machine. The person seems to be in the process of retrieving or organizing the bowling balls from the machine, with the background elements, including the wooden lane surface and colored lights, remaining consistent throughout this sequence.",
#    ]

#     with torch.no_grad():
#         rewards = inferencer.reward(video_paths, prompts, use_norm=True)
#         print(rewards)

